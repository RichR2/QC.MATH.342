---
title: "Lab 3"
author: Richard Romano
output: pdf_document
date: "11:59PM Saturday, February 22, 2020"
---

## Review from Lab 2

You have a set of names divided by gender (M / F) and generation (Boomer / GenX / Millenial):

* M / Boomer      "Theodore, Bernard, Gene, Herbert, Ray, Tom, Lee, Alfred, Leroy, Eddie"
* M / GenX        "Marc, Jamie, Greg, Darryl, Tim, Dean, Jon, Chris, Troy, Jeff"
* M / Millennial  "Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis"
* F / Boomer      "Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred"
* F / GenX        "Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi"
* F / Millennial  "Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne"

Create a list-within-a-list that will intelligently store this data.

```{r}
set <- list("M" = list(), "F" = list())
set$M$Boomer = c("Theodore", "Bernard", "Gene", "Herbert", "Ray", "Tom", "Lee", "Alfred", "Leroy", "Eddie")
set$M$GenX = c("Marc", "Jamie", "Greg", "Darryl", "Tim", "Dean", "Jon", "Chris", "Troy", "Jeff")
set$M$Millennial = strsplit("Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis", split = ", ")[[1]]
set$F$Boomer = strsplit("Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred", split = ", ")[[1]]
set$F$GenX = strsplit("Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi", split = ", ")[[1]]
set$F$Millennial = strsplit("Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne", split = ", ")[[1]]

set
```


Imagine you are running an experiment with many manipulations. You have 14 levels in the variable "treatment" with levels a, b, c, etc. For each of those manipulations you have 3 submanipulations in a variable named "variation" with levels A, B, C. Then you have "gender" with levels M / F. Then you have "generation" with levels Boomer, GenX, Millenial. Then you will have 6 runs per each of these groups. In each set of 6 you will need to select a name without duplication from the appropriate set of names (from the last question). Create a data frame with columns treatment, variation, gender, generation, name and y that will store all the unique unit information in this experiment. Leave y empty because it will be measured as the experiment is executed.

```{r}
n = 14 * 3 * 2 * 3 * 10
X = data.frame(treatment = rep(NA,n), 
               variation = rep(NA,n),
               gender = rep(NA,n),
               generation = rep(NA,n),
               name = rep(NA,n),
               y = rep(NA, n))
X$treatment = rep(letters[1:14], each = n / 14)
X$variation = rep(rep(LETTERS[1:3], each = n / 14 / 3) , times = 14)
X$gender = rep(rep(c("M", "F"), each = n / 14 / 3 / 2), times = 14*3)
X$generation = rep(rep(c("Boomer", "GenX", "Millenial"), each = n / 14/ 3 /2 / 3), times = 14 * 3 * 2)
X$name = rep(unlist(set), times = 14 * 3)
X
```

## Packages

Install the package `pacman` using regular base R.

```{r}
install.packages("pacman")
```


First, install the package `testthat` (a widely accepted testing suite for R) from https://github.com/r-lib/testthat using `pacman`. If you are using Windows, this will be a long install, but you have to go through it for some of the stuff we are doing in class. LINUX (or MAC) is preferred for coding. If you can't get it to work, install this package from CRAN (still using `pacman`), but this is not recommended long term.

```{r}
pacman::p_load(testthat)
```

* Create vector `v` consisting of all numbers from -100 to 100 and test using the second line of code:

```{r}
v= seq(-100, 100)
expect_equal(v, -100 : 101)
```

If there are any errors, the `expect_equal` function will tell you about them. If there are no errors, then it will be silent.

Test the `my_reverse` function from lab2 using the following code:

```{r}
expect_equal(my_reverse(v), rev(v))
expect_equal(my_reverse(c("A", "B", "C")), c("C", "B", "A"))
```

## Basic Binary Classification Modeling

* Load the famous `iris` data frame into the namespace. Provide a summary of the columns and write a few descriptive sentences about the distributions using the code below and in English.

```{r}
data("iris")
iris

```

TO-DO

The outcome metric is `Species`. This is what we will be trying to predict. However, we only care about binary classification between "setosa" and "versicolor" for the purposes of this exercise. Thus the first order of business is to drop one class. Let's drop the data for the level "virginica" from the data frame.

```{r}

iris = iris[iris$Species != "virginica", ]
table(iris$Species)

iris
```

Now create a vector `y` that is length the number of remaining rows in the data frame whose entries are 0 if "setosa" and 1 if "versicolor".

```{r}
y = as.numeric(iris$Species == "versicolor")
y
```

* Write a function `mode` returning the sample mode.

```{r}
#online 
#r_bloggers
test_mode = function(x){ 
    vals = table(x)
    max_vals = max(vals)
    if (all(vals == max_vals))
         mod = NA
    else
         if(is.numeric(x))
     mod = as.numeric(names(vals)[vals == max_vals])
    else
         mod = names(vals)[vals == max_vals]
    mod
}

```

* Fit a threshold model to `y` using the feature `Sepal.Length`. Write your own code to do this. What is the estimated value of the threshold parameter? What is the total number of errors this model makes?

```{r}
n = nrow(iris)
num_errors = array(NA, n)

for (i in 1 : n) {
  y_hat = as.numeric(iris$Sepal.Length > iris$Sepal.Length[i])
  num_errors[i] = sum(y_hat != y)
}
threshold = iris$Sepal.Length[which.min(num_errors)] 

g = function(x) {
  as.numeric(x > threshold)
}

sum(g(iris$Sepal.Length) != y)
table(y)

```

Does Robin's threshold model's performance make sense given the following summaries:

```{r}
threshold
summary(iris[iris$Species == "setosa", "Sepal.Length"])
summary(iris[iris$Species == "versicolor", "Sepal.Length"])
```

Write your answer here in English.

Robin's threshhold model's performance makes sense because the median of "Setosa" is belong his predicted 5.4 length threshold and the average for "Versicolor" is above his 5.4 length threshold.

Create the function `g` explicitly that can predict `y` from `x` being a new `Sepal.Length`.

```{r}
g = function(x) {
  as.numeric(x > threshold)
}
```

* What is the total number of errors this model makes in the dataset $\mathbb{D}$?

```{r}
sum(g(iris$Sepal.Length) != y)
```


## Perceptron

You will code the "perceptron learning algorithm". Take a look at the comments above the function. This is standard "Roxygen" format for documentation. Hopefully, we will get to packages at some point and we will go over this again. It is your job also to fill in this documentation.

```{r}
#' RR_PLA
#'
#' TO-DO: Explain what this function does in a few sentences
#'
#' @param Xinput      Xinput is the training data.
#' @param y_binary    y_binary is the classifiers. the labels {0,1}. Some people will program with {-1,1}
#' @param MAX_ITER    The maximum number of iterations the algorithim performs
#' @param w           the weight vector which is the normal vector to the hyperplane. 
#'
#' @return            The computed final parameter (weight) as a vector of length p + 1
#' @export            [In a package, this documentation parameter signifies this function becomes a public method. Leave this blank.]
#'
#' @author            [your name]

perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w = NULL){
   X2 = cbind(1, Xinput)
    if(is.null(w))w = rep(0, ncol(X2))
      for (iter in 1 : MAX_ITER){
      for (i in 1 : nrow(X2)){
        x_i = X2[i, ]
  yhat_i = ifelse(sum(x_i * w >= 0),1, 0)
 for(j in 1 : ncol(X2)){
 w[j] = w[j] + (y_binary[i] - yhat_i)*X2[i,j]
      
      }
    }
  
  }
      w
}

```

To understand what the algorithm is doing - linear "discrimination" between two response categories, we can draw a picture. First let's make up some very simple training data $\mathbb{D}$.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. Thus, I will write this code for you and you will just run it. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
library(ggplot2)
```

We are going to just get some plots and not talk about the code to generate them as we will have a whole unit on visualization using `ggplot2` in the future.

Let's first plot $y$ by the two features so the coordinate plane will be the two features and we use different colors to represent the third dimension, $y$.

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

TO-DO: #This is a scatterplot with 6 points of training data and is the data is linearly seperable, therefore the perceptron will converge if run on this data. 

Now, let us run the algorithm and see what happens:

```{r}
w_vec_simple_per = perceptron_learning_algorithm(
  cbind(Xy_simple$first_feature, Xy_simple$second_feature),
  as.numeric(Xy_simple$response == 1))
w_vec_simple_per
```

Explain this output. What do the numbers mean? What is the intercept of this line and the slope? You will have to do some algebra.
#These numbers are the assigned weights to the 3 weights w0, w1, w2 for this model. 
#slope equals -.002
#y intercept = 1499 (defintley not right)


```{r}
simple_perceptron_line = geom_abline(
    intercept = w_vec_simple_per[1] / w_vec_simple_per[3], 
    slope = w_vec_simple_per[2] / w_vec_simple_per[3], 
    color = "orange")
simple_viz_obj + simple_perceptron_line
```

Explain this picture. Why is this line of separation not "satisfying" to you?

#Because the line is weak in the sense that, to predict future points that are right next to the line could be classified 0 but are in actuality closer to the 1 responses, and if we had a hyperplane with a even margin between points it would 'correctly" predict those points 1's. Therefore there exist points that sit on my hyperplane and  right to the other side would be classified differnt then the points right next to them. In other words if I were to use an algoritihm such as KNN I would get a completley different prediciton for the same future points in some areas. Now this is not a rule, it is just an example portraying that the line is weak and needs to be more towards the middle, something with a slope of -1 with an intercept that looks like 4 if those units are correct. Needs to be in between the 2 l lines, l(up) and l(down) (not shown) and with lineraly sepreability there should be a middle where the l is between both of these lines at equal length.

For extra credit, program the maximum-margin hyperplane perceptron that provides the best linear discrimination model for linearly separable data. Make sure you provide ROxygen documentation for this function.

```{r}
#TO-DO
```


## Support Vector Machine


```{r}
X_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
y_binary = as.numeric(Xy_simple$response == 1)
```

Use the `e1071` package to fit an SVM model to `y_binary` using the features in `X_simple_feature_matrix`. Do not specify the $\lambda$ (i.e. do not specify the `cost` argument). Call the model object `svm_model`. Otherwise the remaining code won't work.

```{r}
library(e1071)
svm_model = svm(X_simple_feature_matrix, y_binary) 

  
```

and then use the following code to visualize the line in purple:

```{r}
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% X_simple_feature_matrix[svm_model$index, ] # the other terms
)
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_perceptron_line + simple_svm_line
```

Is this SVM line a better fit than the perceptron?

No. It is the same line.

3. Now write pseuocode for your own implementation of the linear support vector machine algorithm using the Vapnik objective function we discussed.



Note there are differences between this spec and the perceptron learning algorithm spec in question \#1. You should figure out a way to respect the `MAX_ITER` argument value. 



```{r}
#' Support Vector Machine 
#
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  #using y = {-1,1}
  for(1:MAX_ITER) #loop through iterations
  #minimize w_length subject to 
    y_i(w * x_i - b) >= 1
  #max(0, 1 - y_i(sum(w * x_i - b)))
    if minimze = 0 
    no hinge loss.
    if minize != 0
    minimize [1/n(sum(max(0, 1 - yi(w * x_i -b))))] + lambda(w_length)^2
    
    
  
}
```


If you are enrolled in 390 the following is extra credit but if you're enrolled in 650, the following is required. Write the actual code. You may want to take a look at the `optimx` package we discussed in class. You can feel free to define another function (a "private" function) in this chunk if you wish. R has a way to create public and private functions, but I believe you need to create a package to do that (beyond the scope of this course).

```{r}
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  #TO-DO
}

```

If you wrote code (the extra credit), run your function using the defaults and plot it in brown vis-a-vis the previous model's line:

```{r}
svm_model_weights = linear_svm_learning_algorithm(X_simple_feature_matrix, y_binary)
my_svm_line = geom_abline(
    intercept = svm_model_weights[1] / svm_model_weights[3],#NOTE: negative sign removed from intercept argument here
    slope = -svm_model_weights[2] / svm_model_weights[3], 
    color = "brown")
simple_viz_obj  + my_svm_line
```

Is this the same as what the `e1071` implementation returned? Why or why not?

4. Write a $k=1$ nearest neighbor algorithm using the Euclidean distance function. Respect the spec below:

```{r}
#' This function implements the nearest neighbor algorithm.
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param Xtest       The test data that the algorithm will predict on as a n* x p matrix.
#' @return            The predictions as a n* length vector.


## THIS WAS NOT MY CODE. ONLINE dataaspirsant_com gets credit


euclidean_dist = function(x, y){
  dist = 0
  for(i in c(1:(length(x) - 1))){
    dist = dist + (x[[i]] - y[[i]])^2
  }
dist = sqrt(dist)
dist
}

nn_algorithm_predict = function(Xinput, y_binary, Xtest){
  pred = c()
  for(i in c(1:nrow(Xinput))){
    euc_dist = c()
    eu_char = c()
    label_good = 0 
    label_bad = 0
  }
for(j in c(1 : nrow(y_binary))){
  euc_dist = c(euc_dist, eucledian_dist(Xinput[i,], Y_binary[j,] ))
  eu_char = c(eu_char, as.character(y_binary[j,]))
  
}
  eu = data.frame(eu_char,euc_dist)
  eu = eu[order(eu$euc_dist),]
  eu = eu[1 : Xtest, ]

for(k in c(1:nrow(eu)))
 if(as.character(eu[k, "eu_char"]) =="g"){
   label_good = label_good +1
 }else{
   label_bad = label_good + 1
 }
if(label_good > label_bad){
  pred = c(pred, "g")
}else if(label_bad > label_good)
  pred = c(pred,"b")


return(pred)
}
```


Write a few tests to ensure it actually works:

```{r}


```

We now add an argument `d` representing any legal distance function to the `nn_algorithm_predict` function. Update the implementation so it performs NN using that distance function. Set the default function to be the Euclidean distance in the original function. Also, alter the documentation in the appropriate places.

```{r}
## THIS WAS NOT MY CODE. ONLINE dataaspirsant_com gets credit
#I added function distance inside the function. 

nn_algorithm_predict = function(Xinput, y_binary, Xtest, d){
  
  d = function(x, y){
  d = 0
  for(i in c(1:(length(x) - 1))){
    d = d + (x[[i]] - y[[i]])^2
  }
d = sqrt(dist)
d
}
  pred = c()
  for(i in c(1:nrow(Xinput))){
    euc_dist = c()
    eu_char = c()
    label_good = 0 
    label_bad = 0
  }
for(j in c(1 : nrow(y_binary))){
  euc_dist = c(euc_dist, eucledian_dist(Xinput[i,], Y_binary[j,] ))
  eu_char = c(eu_char, as.character(y_binary[j,]))
  
}
  eu = data.frame(eu_char,euc_dist)
  eu = eu[order(eu$euc_dist),]
  eu = eu[1 : k_value, ]

for(k in c(1:nrow(eu)))
 if(as.character(eu[k, "eu_char"]) =="g"){
   label_good = label_good +1
 }else{
   label_bad = label_good + 1
 }
if(label_good > label_bad){
  pred = c(pred, "g")
}else if(label_bad > label_good)
  pred = c(pred,"b")


return(pred)
}
```




For extra credit (unless you're a masters student), add an argument `k` to the `nn_algorithm_predict` function and update the implementation so it performs KNN. In the case of a tie, choose $\hat{y}$ randomly. Set the default `k` to be the square root of the size of $\mathcal{D}$ which is an empirical rule-of-thumb popularized by the "Pattern Classification" book by Duda, Hart and Stork (2007). Also, alter the documentation in the appropriate places.

```{r}
#TO-DO --- extra credit for undergrads
```




---
title: "Lab 6"
author: "Richard Romano"
output: pdf_document
date: "11:59PM March 21, 2020"
---

Load the Boston Housing data and create the vector `y`, design matrix `X` and let `n` and `p_plus_one` be the number of rows and columns.

```{r}
y = MASS::Boston$medv
X = as.matrix(cbind(1, MASS::Boston[, 1 : 13]))
n = nrow(X)
p_plus_one = ncol(X)
?Boston
```

Create a new matrix `Xjunk` by adding random columns to `X` to make the number of columns and rows the same.

```{r}
Xjunk = X
for(j in (p_plus_one + 1) : n){
  Xjunk = cbind(Xjunk, rnorm(n))
  
}
dim(Xjunk)
```

Test that the projection matrix onto $colsp[Xjunk]$ is the same as $I_n$:

```{r}
pacman::p_load(testthat)
I_n = diag(n)
expect_equal(c(Xjunk %*% solve(t(Xjunk) %*% Xjunk) %*% t(Xjunk)), c(I_n))
```

Write a function spec'd as follows:

```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  a_parallel = (v %*% t(v) / sum(v^2)) %*% a
  a_perpendicular = a - a_parallel
  list(a_parallel = a_parallel, a_perpendicular = a_perpendicular)
}
```

Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
#prediction:
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
#prediction:
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7))
t(result$a_parallel) %*% result$a_perpendicular
#prediction:
result$a_parallel + result$a_perpendicular
#prediction:
result$a_parallel / c(1, 3, 5 ,7)
#prediction:
```


Try to orthogonally project onto the column space of $X$ by projecting $y$ on each vector of $X$ individually and adding up the projections. You can use the function `orthogonal_projection`.

```{r}
sumProj <- 0
for (j in 1:p_plus_one){
  sumProj = sumProj + orthogonal_projection(y, X[ , j])$a_parallel
}
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
yhat = lm(y ~ X)$fitted.values
sqrt(sum(sumProj^2)) / sqrt(sum(yhat^2))
```

Convert $X$ into $V$ where $V$ has the same column space as $X$ but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm.

```{r}
V = matrix(NA, nrow = nrow(X), ncol = ncol(X))
V[ , 1] <- X[ , 1]
for (j in 2:p_plus_one){
  V[ , j] <- X[ , j]
  for (k in 1:(j - 1)){
    V[ , j] <- V[ , j] - orthogonal_projection(X[ , j], V[ , k])$a_parallel
  }
}
t(V[ , 1]) %*% V[ , 2]

```

Convert $V$ into $Q$ whose columns are the same except normalized

```{r}
Q = matrix(NA, nrow = nrow(X), ncol = ncol(X))
for(j in 1:p_plus_one){
  Q[ , j] = V[ , j] / sqrt(sum(V[ , j]^2))
}
```

Verify $Q^T Q$ is $I_{p+1}$ i.e. $Q$ is an orthonormal matrix.

```{r}
expect_equal(t(Q) %*% Q, diag(p_plus_one))
```


Project $y$ onto $colsp[Q]$ and verify it is the same as the OLS fit.

```{r}
expect_equal(c(unname(Q %*% t(Q) %*% y)), unname(yhat))
```


Project $Y$ onto the columns of $Q$ one by one and verify it sums to be the projection onto the whole space.

```{r}
sumProj <- 0
for (j in 1:p_plus_one){
  sumProj = sumProj + orthogonal_projection(y, Q[ , j])$a_parallel
}
```

Verify the sum of projections is $\hat{y}$

```{r}
expect_equal(c(sumProj), unname(yhat))
```

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
prop_train = 0.8
n_train = round(prop_train * n)
index_train = sample(1:n, n_train, replace = FALSE)
index_test = setdiff(1:n, index_train)
expect_equal(sort(c(index_test, index_train)), 1:n)

X_train = X[index_train, ]
y_train = y[index_train]
X_test = X[index_test, ]
y_test = y[index_test]

```

Find the $s_e$ in sample and out of sample. Which one is greater? Note: we are now using $s_e$ and not RMSE since RMSE has the $-(p + 1)$ in the denominator which makes comparison more difficult when the $n$'s are different.

```{r}
mod_train = lm(y_train ~ ., data.frame(X_train))
sd(mod_train$residuals)
mod_test = lm(y_test ~ ., data.frame(X_test))
in_sample_se =sd(mod_test$residuals)
y_hat_oos = predict(mod_train, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
oo_se =sd(oos_residuals)

```


Do these two exercises 1,000 times and find the average difference between $s_e$ and oos$s_e$. This is just `sd(e)` the standard deviation of the residuals.

```{r}
for(i in 1:1000){
  mod_train[[i]] = lm(y_train ~ ., data.frame(X_train))
  in_sample_se = sd(mod_train[[i]]$residuals)
  y_hat_oos = predict(mod_test[[i]], data.frame(X_test))
  oos_residuals[i] = y_test - y_hat_oos
  oo_se[[i]] = sd(oos_residuals)
  average_difference = mean(in_sample_se - oo_se)
}
```

Using `Xjunk` from above, divide the data into training and testing sets. Fit the model in-sample and calculate $s_e$ in-sample by varying the number of columns used beginning with the first column. Keep the $s_e$ values in the variable `s_es` which has length $n$. Show that it reaches 0 at $n$ i.e. the model overfits.

```{r}
prop_junk_train = 0.8
n_junk_train = round(prop_junk_train * n)
index_junk_train = sample(1:n, n_junk_train, replace = FALSE)
index_junk_test = setdiff(1:n, index_train)

X_junk_train = Xjunk[index_junk_train, ]
X_junk_test = Xjunk[index_junk_test, ]
X_junk_test
in_sample_s_e = array(NA, n)
for (j in 1 : (n)){
  X_junk_train = cbind(X_train, runif(n_junk_train))
  y_hat_train = predict(mod_train[[j]], data.frame(X_train)) 
  in_sample_s_e[j] = sd(y_train - y_hat_train)

}


```

Do the same thing but now calculate oos$s_e$. Does this go to zero? What is the index corresponding to the best model?

```{r}
oo_sample_s_e = array(NA, n)
for (j in 1 : (n)){
  X_junk_test= cbind(X_test, runif(n - n_junk_train))
  y_hat_test = predict(mod_train[[j]], data.frame(X_test)) 
  oo_sample_s_e[j] = sd(y_test - y_hat_test)
}
```

Beginning with the Boston Housing Data matrix `X`, pull out the second column, the `crim` feature and call it `x2`. Then, use the `cut` function to bin each of its $n$ values into two bins: the first is all values <= the median of `crim` and the second is all values > median of `crim`. Call it `x2bin`. Use the `table` function to ensure that half of the values are in the first group and half in the second group. This requires reading the documentation for `cut` carefully and using the `quantile` function carefully.

```{r}
x2 = X[,2]
x2_bin = cut(x2, breaks=quantile(x2, c(0,.5,1)), include.lowest=TRUE)
table(x2_bin)
```

Now convert the factor variable `x2bin` to two dummies, `X2dummy`, a matrix of $n \times 2$ and verify the rowsums are all 1. They must be 1 because either the value is <= median or > median.

```{r}
X2dummy = model.matrix(~ 0 + ., data.frame(x2_bin))
table(rowSums(X2dummy))
X2dummy
```

Drop the first column of this matrix to arrive at `X2dummyfeatures`.

```{r}
X2dummyfeatures = as.matrix(X2dummy[, -1])
X2dummyfeatures

```


What you did with `crim`, do for all 13 variables in the Boston housing data, ie create `X2dummyfeatures` for all and then column bind them all together into a massive `Xdummy` matrix. Then run a regression of $y$ on those features and report $R^2$.


```{r} 
x3 = X[ , 3]
x3_bin = cut(x3, breaks=unique(quantile(x3, c(0,.5,1))), include.lowest=TRUE)
x4 = X[ , 4]
x4_bin = cut(x4, breaks=unique(quantile(x4, c(0,.5,1))), include.lowest=TRUE)
x5 = X[ , 5]
x5_bin = cut(x5, breaks=unique(quantile(x5, c(0,.5,1))), include.lowest=TRUE)
x6 = X[ , 6]
x6_bin = cut(x6, breaks=unique(quantile(x6, c(0,.5,1))), include.lowest=TRUE)
x7 = X[ ,7]
x7_bin = cut(x7, breaks=unique(quantile(x7, c(0,.5,1))), include.lowest=TRUE)
x8 = X[ , 8]
x8_bin = cut(x8, breaks=unique(quantile(x8, c(0,.5,1))), include.lowest=TRUE)
x9 = X[ , 9]
x9_bin = cut(x9, breaks=unique(quantile(x9, c(0,.5,1))), include.lowest=TRUE)
x10 = X[ , 10]
x10_bin = cut(x10, breaks=unique(quantile(x10, c(0,.5,1))), include.lowest=TRUE)
x11 = X[ , 11]
x11_bin = cut(x11, breaks= quantile(x11, c(0,.5,1)), include.lowest=TRUE)
x12 = X[ , 12]
x12_bin = cut(x12, breaks=unique(quantile(x12, c(0,.5,1))), include.lowest=TRUE)
x13 = X[ , 13]
x13_bin = cut(x13, breaks=unique(quantile(x13, c(0,.5,1))), include.lowest=TRUE)
x14 = X[ , 14]
x14_bin = cut(x14, breaks=unique(quantile(x14, c(0,.5,1))), include.lowest=TRUE)
X3dummy = factor(X[, 3])

X4dummy = as.matrix(X4dummy[ , -1])
X5dummy = factor(X[ , 5])
X4dummy = model.matrix(~ 0 + ., data.frame(x4_bin))
table(rowSums(X4dummy))
X4dummy = as.matrix(X4dummy[ , -1])
X6dummy = model.matrix(~ 0 + ., data.frame(x6_bin))
table(rowSums(X6dummy))
X6dummy = as.matrix(X6dummy[ , -1])
X6dummy
X7dummy = model.matrix(~ 0 + ., data.frame(x7_bin))
table(rowSums(X7dummy))
X7dummy = as.matrix(X7dummy[ , -1])
X8dummy = model.matrix(~ 0 + ., data.frame(x8_bin))
table(rowSums(X8dummy))
X8dummy = as.matrix(X8dummy[ , -1])
X9dummy = model.matrix(~ 0 + ., data.frame(x9_bin))
table(rowSums(X7dummy))
X9dummy = as.matrix(X9dummy[ , -1])
X10dummy = model.matrix(~ 0 + ., data.frame(x10_bin))
table(rowSums(X10dummy))
X10dummy = as.matrix(X10dummy[ , -1])
X11dummy = model.matrix(~ 0 + ., data.frame(x11_bin))
table(rowSums(X11dummy))
X11dummy = as.matrix(X11dummy[ , -1])
X12dummy = model.matrix(~ 0 + ., data.frame(x12_bin))
table(rowSums(X12dummy))
X12dummy = as.matrix(X12dummy[ , -1])
X13dummy = model.matrix(~ 0 + ., data.frame(x13_bin))
table(rowSums(X13dummy))
X13dummy = as.matrix(X13dummy[ , -1])
X14dummy = model.matrix(~ 0 + ., data.frame(x14_bin))
table(rowSums(X14dummy))
X14dummy = as.matrix(X14dummy[ , -1])
Xdummyfeatures_all = cbind(X2dummyfeatures,X3dummy,X4dummy,X5dummy,X6dummy,X7dummy,X8dummy,X9dummy,X10dummy,X11dummy,X12dummy,X13dummy,X14dummy)
boston_dummy_regress = lm(y ~ Xdummyfeatures_all)
boston_dummy_regress
boston_dummy_regress$rsquared
```

This time create two dummies for each variable: (1) between the 33%ile and 66%ile and (2) greater than the 66%ile. Run the regression on all dummies for all variables and report $R^2$. Hint: you do not need to go through the exercise of creating the dummy columns manually; use `factor` instead. Then use `lm` to run the regression (do not do it manually using the $X$ matrices).

```{r}
#TODO
```

Keep doing this until each continuous variable has 31 dummies for a large final $p$. Report all $R^2$;s. Why is it increasing and why is the last one so high?

```{r}
#TODO
```

Repeat this exercise with a 20% test set held out. Record in sample $s_e$'s and oos$s_e$'s. Do we see the canonical picture?

```{r}
#TODO
```

What is the optimal number of bins (dummies) for each feature? Worded another way, what is the optimal complexity model among this set modeling strategy (binning)?

```{r}
#TODO
```







